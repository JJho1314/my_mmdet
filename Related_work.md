# Related Work

## 1. Open-Vocabulary Object Detection Using Captions 

OVD是一种基于Caption的目标检测方法，其主要过程包括以下几个步骤：

1. Caption分割：从整个视频序列中自适应地分割出每句Caption，并生成每句Caption的产生时间戳。
2. 阅读理解：利用自然语言处理技术，将每个Caption转换为一组关键词或短语，并为每个关键词或短语分配权重。
3. 视觉检测：使用计算机视觉算法，包括弱监督目标检测、自增强目标检测等技术，自动确定每个关键词或短语的位置，并生成一个感兴趣的区域（ROI）列表。
4. 交叉验证：将视觉检测的结果与原始Caption的文本语意进行交叉验证，以消除误报或漏报的ROI。
5. 组合检测：对于由多个ROI组成的单个物体，将其组合成一个检测到的物体，然后将所有检测到的物体和句子级别的文本信息整合在一起。
6. 检索：基于整合后的信息，选择适当的检索算法，从多个数据库中检索相关的实体信息，并为每个检测到的实体分配一个唯一的标识符，以便后续跟踪和分析。

综上所述，OVD方法主要包括基于文本的Caption分割、基于视觉目标检测的ROI提取、交叉验证、组合检测和基于检索的实体分析等技术。它可以应用于开放词汇场景，实现从Caption到实体的快速检测和分析，是一种比较普及的目标检测方法。

## 2. Open-vocabulary Object Detection via Vision and Language Knowledge Distillation

ViLD（Visual-Language Detection）是2021年CVPR会议上提出的一种新型深度学习算法，用于解决视觉和语言联合检测问题。该算法借鉴了自然语言处理领域中关于多模态数据的最新进展，并将其应用于图像和语言的联合检测。

ViLD算法的主要思想是从视觉和语言两个方面来理解图像，其中对于视觉数据，ViLD采用了基于骨干网的对象检测方法（如Faster  R-CNN），对目标进行检测和定位。对于语言数据，ViLD使用transformer编码器将语句编码为向量，帮助找到与概括语句中意义相同的对象。

具体来说，ViLD算法主要包括两部分：前期训练和特定任务的目标检测模型微调。前期训练时，ViLD算法使用图像和语言数据对模型进行训练，使其能够直接使用语言指令和图像信息，完成联合检测任务。

在特定任务的微调过程中，ViLD沿用了Faster R-CNN的训练机制，通过对部分网络层进行冻结，然后使用一个小学习率进行训练，并且采取了一些细节改进措施来提高检测效果。从实验结果看，ViLD算法的检测准确率在多个数据集上都取得了显著提高。

总而言之，ViLD算法以视觉和语言联合识别为核心思想，结合了最新的自然语言处理技术，利用实时生成的约束来优化图像识别。它是当前多模态深度学习领域的研究热点之一，也是实现图像和语言联合检测的一种有效方法。

## 3. Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model

总的来说这是一篇基于 ViLD 的改进性质的文章，主要从prompt改进，对background改进以及对前景的embedding_text建模进行改进这三个方面解决已有的问题。

首先是text encoder 生成类别表示的处理，文章使用了 CoOp 代替了 原本的固定的模板。总的来说  CoOp 原始的文章的目标就是利用 CLIP 进行开放式图片类别的表示 embedding_text , 用CLIP 的 image  encoder 生成图片的embedding_image, 然后计算图片的归类，而基本的思路就是将预设的模板改进为可学习的参数向量。

对目标检测中背景类的处理，第一种处理方式：作者这里的做法是，对于一个背景的候选框，它应该“和谁都不像”，即它属于任何类别的概率应该是一个均匀分布。即让背景类别属于某一类的概率尽可能的等于$1/|CB|$（$|CB|$为训练集（基类）里面的类别数）。第二种处理方式：作者提出使用另外一种背景候选框的处理，就是让背景候选框对应的背景prompt $V_{bg}$和基本别$V_c$的prompt一样是可以学习得到的

对一直类别前景的处理，作者认为前景的候选框与真实边界框之间的IOU值是存在差别的，即一些前景的候选框并不总是高IOU的，因为前景的候选框可能只是包含了前景的一部分。因此，对于不同IOU值的前景的候选框应该分别处理，即作者提出的context grading scheme。，对于每个类别，如person，为不同IOU水平的前景的候选框生成不同的text_embedding，因此，对于生成的前景的候选框，可以按照和真实边界框（GT Box）的IOU划分到一个一个的区间，对于在不同区间内的前景的候选框，使用不同的可学习的prompt engineering。第一种和第二种处理方式的主要区别在于第二种实际上创建了一个背景类别的标签。关于这两种方式，作者指出第二种方式不如第一种。因为背景内容可能会有很大的差异，第二种方法是学习一个显式的背景emmbeding，让所有的背景候选框都接近它，这是不够的。相反，在第一种方式中，它被隐式地解释为，让每个背景候选框都远离所有其他的类的emmbeding，这可以更健壮。

## 4. RegionCLIP: Region-based Language-Image Pretraining

RegionCLIP是一种新的目标检测算法，它基于CLIP模型  (Contrastive Language-Image  Pre-Training)。CLIP模型是OpenAI于2021年首次公开发表的一个新型视觉语言模型。CLIP模型允许用户通过直接使用文本和图像的特征，进行自然语言问题答案、图像分类、目标检测等相关任务。

RegionCLIP算法的主要创新之处在于它采用了一种新的策略来解决目标检测的网格化问题，即在使用目标检测模型检测图像之前，使用CLIP模型将图像与单词进行对齐，这样可以将单词映射为图像的一部分。这样，对于图像中的每个单元格，不再是只对应一个个预测框，而是对应一组由CLIP映射的关键词的注意力分布，这一点通过把关键词添加到固定大小的小区域中来实现。这样就可以嵌入一些高级语义信息，而不仅仅是图像的几何结构。

具体来说，RegionCLIP的目标检测过程可以分为以下几个步骤：

1. 将图像和关键词输入到CLIP模型中，获取对应的特征。
2. 根据特征生成一组关键字的位置分布，并将这些分布嵌入到固定大小的小区域中。
3. 在固定大小的小区域中使用CNN，得到固定数目的位置粗略框。
4. 使用逐点注意力机制结合粗略框，得到更精细的精细框，这些框对应于每个固定大小的小区域内的关键字。
5. 根据预测框的得分选择最佳候选框。

总之，RegionCLIP算法通过结合CLIP模型和目标检测模型的方法，充分利用了文本和视觉信息，提供了一种强大的目标检测框架。它在多个数据集上取得了优秀的检测效果，并已成为当前目标检测领域的研究热点之一。

